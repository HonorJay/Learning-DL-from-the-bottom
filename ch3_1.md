# ch3. 신경망

앞 장에서 배운 퍼셉트론은 복잡한 함수도 표현할 수 있다는 장점이 있지만, 가중치와 편향을 사람이 직접 설정해야한다는 단점이 있음. 

가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력은 신경망의 중요한 성질.



## 3.1 퍼셉트론에서 신경망으로

### 3.1.1 신경망의 예

신경망은 다층 퍼셉트론처럼 입력층과 출력층, 그리고 그 두 층 사이의 은닉층으로 이루어져 있음.

은닉층의 뉴런은 입력층이나 출력층과 달리 사람 눈에 보이지 않음. 



### 3.1.2 퍼셉트론 복습

$$
y = h(b + w_1x_1 + w_2x_2)
$$

입력 신호의 총합이 h(x)라는 함수를 거쳐 변환되어, 그 변환된 값이 y의 출력이 됨.
$$
(x ≤ 0) → h(x) = 0
$$

$$
(x > 0) → h(x) = 1
$$

h(x) 함수는 입력이 0을 넘으면 1을 돌려주고 그렇지 않으면 0을 돌려줌.



### 3.1.3 활성화 함수의 등장

위 식의 h(x) 함수처럼 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 활성화 함수(activation function)라 함.

활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 함
$$
y = h(b + w_1x_1 + w_2x_2)
$$

위 식은 아래의 두 식처럼 <u>가중치가 곱해진 입력 신호의 총합을 계산하는 단계</u>와 <u>그 합을 활성화 함수에 입력해 결과를 내는 단계</u>로 두 단계로 나누어 질 수 있음.
$$
a = b + w1x1 + w2x2
$$

$$
y = h(a)
$$

가중치 신호를 조합한 결과가 a라는 노드가 되고, 활성화 함수 h()를 통과하여 y하는 노드로 변환됨. 

(*이 책에서는 뉴런과 노드라는 용어를 같은 의미로 사용함)



## 3.2 활성화 함수

활성화 함수는 임계값(θ, theta)을 경계로 출력이 바뀌는데, 이런 함수를 **계단 함수**(step function)라 함.

### 3.2.1 시그모이드 함수

다음은 신경망에서 자주 이용하는 활성화 함수인 시그모이드 함수(sigmoid function)를 나타낸 식.
$$
h(x)= \frac {1}{1+e^{-x}}
$$
e는 자연상수로 2.7182...의 값을 갖는 실수. 

시그모이드 함수에 1.0과 2.0을 입력하면 h(1.0) = 0.731..., h(2.0) = 0.880... 처럼 특정 값을 출력.



신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달. 그 외에 뉴런이 여러 층으로 이어지는 구조와 신호를 전달하는 방법은 기본적으로 앞에서 살펴본 퍼셉트론과 같음. 



## 3.2.2 계단 함수 구현하기

계단 함수는 다음과 같이 입력이 0을 넘으면 1을 출력하고, 그 외에는 0을 출력하는 함수.

``` python
def step_function(x):
    if x > 0:
        return 1
    else:
        return 0
```

이 구현은 단순하고 쉽지만, 인수 x는 실수(부동소수점)만 받아들임.

즉, 넘파이 배열을 인수로 넣을 수는 없음. 

따라서 앞으로를 위해 넘파이 배열도 지원하는 다음과 같은 구현이 필요함

```python
def step_function(x):
    y = x > 0
    return y.shape(np.int)
```

``` python
>>> import numpy as np
>>> x = np.array([-1.0, 1.0, 2.0])
>>> x
array([-1., 1., 2.])

>>> y = x > 0
>>> y
array([False, True, True], dtype = bool)
```

넘파이 배열에 부등호 연산을 수행하면 배열의 원소 각각에 부등호 연산을 수행한 bool 배열이 생성 됨.

위 인터프리터에서 y는 bool 배열. 이를 0이나 1을 출력하는 함수로 바꾸기 위해 아래와 같은 작업이 필요.

``` python
>>> y = y.astype(np.int)
>>> y
array([0, 1, 1])
```

이처럼 넘파이 배열의 자료형을 변환할 때는 astype() 메서드를 이용. 원하는 자료형을 인수로 지정.

파이썬에서 bool을 int로 변환하면 True는 1로, False는 0으로 변환됨.



### 3.2.3 계단 함수의 그래프

``` python
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5까지 0.1 간격을 둔 배열
    y = step_function(x)
    plt.plot(x, y)
    plt.ylim(-0.1, 1.1) # y축의 범위 지정
    plt.show()
```



### 3.2.4 시그모이드 함수 구현하기

$$
h(x)= \frac {1}{1+e^{-x}}
$$

시그모이드 함수는 파이썬으로 다음과 같이 작성

``` python
def sigmoid(x):
    return 1/(1+np.exp(-x))
```

np.exp(-x)는 자연상수 e의 -x제곱에 해당함. 



``` python
>>> x = np.array([-1.0, 1.0, 2.0])
>>> sigmoid(x)
array([0.26894142, 0.73105858, 0.88079708])
```

이 함수가 넘파이 배열도 처리해줄 수 있는 이유는 넘파이의 브로드캐스트 덕분.

브로드캐스트 기능이란 넘파이 배열과 스칼라값의 연산을 넘파이 배열의 원소 각각과 스칼라값의 연산으로 바꿔 수행하는 것.

```python
>>> t = np.array([1.0, 2.0, 3.0])
>>> 1.0 + t
array([2., 3., 4.])

>>> 1.0 / t
array([1.		, 0.5		, 0.33333333])
```

결과적으로 스칼라값과 넘파이 배열의 각 원소 사이에서 연산이 이뤄지고, 연산 결과가 넘파이 배열로 출력.

앞에서 구현한 sigmoid 함수에서도 np.exp(-x)가 넘파이 배열을 반환하기 때문에 1 / (1+np.exp(-x))도 넘파이 배열의 각 원소에 연산을 수행한 결과를 내어 줌.

```python
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # y축 범위(limit) 지정
plt.show()
```



### 3.2.5 시그모이드 함수와 계단 함수 비교

시그모이드 함수와 계단 함수의 가장 큰 차이는 매끄러움.

- 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변화함.

- 계단 함수는 0을 경계로 출력이 바뀜. 

- 계단 함수는 0과 1 중 하나의 값만 반환함.
- 시그모이드 함수는 실수를 반환함.
- 두 함수는 매끄러움이라는 점에서는 다르지만 크게보면 같은 모양을 하고 있음. 둘 다 입력이 작을 때의 출력은 0에 가깝고(혹은 0이고), 입력이 커지면 출력이 1에 가까워지는(혹은 1이 되는) 구조.
- 입력이 아무리 작거나 커도 출력은 0에서 1사이.



### 3.2.6 비선형 함수

시그모이드 함수와 계단 함수의 중요한 공통점은 둘 모두 **비선형 함수**임.

시그모이드 함수는 곡선, 계단 함수는 계단처럼 구부러진 직선으로 나타나지만 공통적으로 비선형 함수로 분류됨.

신경망에서는 활성화 함수로 비선형 함수를 사용해야 함. 

선형 함수를 이용해서는 여러 층으로 구성하는 이점을 살릴 수 없음.



### 3.2.7 ReLU함수

신경망 분야에서는 오래전부터 시그모이드 함수를 이용해왔지만, 최근에는 ReLU(Rectified Linear Unit) 함수를 주로 이용.

ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하면 0을 출력.
$$
(x > 0) → h(x) = x
$$

$$
(x ≤ 0) → h(x) = 0
$$

``` python
def relu(x):
    return np.maximun(0, x)
```

maximum 함수는 두 입력 값 중 큰 값을 선택해 반환하는 함수.