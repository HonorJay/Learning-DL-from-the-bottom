## 3.5 출력층 설계하기

신경망은 분류와 회귀 모두에 이용할 수 있음. 다만 둘 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 다름. 일반적으로 회귀에는 항등 함수를, 분류에는 소프트맥스 함수를 사용. 

| NOTE_<br />기계학습 문제는 분류(classification)와 회귀(regression)로 나뉨. 분류는 데이터가 어느 클래스(class)에 속하느냐는 문제. 예를들어 사진 속 인물의 성별을 분류하는 문제. 회귀는 입력 데이터에서 (연속적인) 수치를 예측하는 문제. 예를들어 사진 속 인물의 몸무게를 예측하는 문제. |
| ------------------------------------------------------------ |



### 3.5.1 항등 함수와 소프트맥스 함수 구현하기

**항등 함수**(identity function)는 입력을 그대로 출력. 입력과 출력이 항상 같다는 뜻의 항등.

출력층에서 항등 함수를 사용하면 입력 신호가 그대로 출력 신호가 됨.



분류에서 사용하는 **소프트맥스 함수**(softmax function)의 식은 다음과 같음.

![소프트맥스 함수](https://raw.githubusercontent.com/HonorJay/images/image/img/소프트맥스 함수.gif)

exp(x)는 $e^x$을 뜻하는 지수 함수(exponential function)임(e는 자연상수). n은 출력층의 뉴런 수, $y_k$는 그 중 k번째 출력임을 뜻함. 

소프트맥스의 출력은 모든 입력 신호로부터 영향을 받음. 



소프트맥스 함수를 파이썬으로 표현한 예

```python
>>> a = np.array([0.3, 2.9, 4.0])

>>> exp_a = np.exp(a) # 지수 함수
>>> print(exp_a)
[ 1.349853881	18.17414537, 54.59815003]

>>> sum_exp_a = np.sum(exp_a) # 지수 함수의 합
>>> print(sum_exp_a)
74.1221542102

>>> y = exp_a / sum_exp_a
>>> print(y)
[ 0.01821127 0.2451918 0.73659691]
```



``` python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a/sum_exp_a
    
    return y
```



### 3.5.2 소프트맥스 함수 구현 시 주의점

softmax() 함수의 코드는 컴퓨터로 계산할 때 오버플로 문제라는 결함이 있음. 

소프트맥스 함수는 지수 함수를 사용하는데, 지수 함수가 너무 큰 수를 반환할 때 큰 값 끼리 나눗셈을 하면 결과 수치가 불안정해짐.

![softmax_function2](https://raw.githubusercontent.com/HonorJay/images/image/img/softmax_function2.gif)

![softmax_function3](https://raw.githubusercontent.com/HonorJay/images/image/img/softmax_function3.gif)

![softmax_function4](https://raw.githubusercontent.com/HonorJay/images/image/img/softmax_function4.gif)

```python
>>> a = np.array([1010, 1000, 990])
>>> np.exp(a) / np.sum(np.exp(a))
array([ nan, nan, nan])
```

```python
>>> c = np.max(a)
>>> a - c
array([0, -10, -20])

>>> np.exp(a-c) / np.sum(np.exp(a-c))
array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])
```

아무런 조치 없이 계산한 위의 식은 nan 값을 출력하는 반면, 입력 신호 중 최대값을 빼준 아래 식은 올바르게 계산된 값이 출력됨.

이를 바탕으로 소프트맥스 함수를 다시 구현한 예.

```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
```



### 3.5.3 소프트맥스 함수의 특징

softmax() 함수를 사용하면 신경망의 출력은 다음과 같이 계산할 수 있음.

```python
>>> a = np.array([0.3, 2.9, 4.0])
>>> y = softmax(a)
>>> print(y)
[ 0.01821127	0.24519181	0.73659691]

>>> np.sum(y)
1.0
```



소프트맥스 함수의 출력은 0에서 1.0 사이의 실수.

소프트맥스 함수 출력의 총합은 1. 

출력의 총합이 1이 된다는 점은 소프트맥스 함수의 중요한 성질.

이 성질 덕분에 소프트맥스 함수의 출력을 '확률'로 해석할 수 있음



앞의 예에서 y[0]의 확률은 0.018(1.8%), y[1]의 확률은 0.245(24.5%), y[2]의 확률은 0.737[73.7%]로 해석할 수 있음. 

이 결과 확률들로부터 '2번째 원소의 확률이 가장 높으니, 답은 2번째 클래스다'라고 할 수 있음.

또는 '74%의 확률로 2번째 클래스, 25%의 확률로 1번째 클래스, 1%확률로 0번째 클래스다'와 같이 확률적인 결론도 낼 수 있음.



여기서 주의점으로 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않음. 이는 지수 함수 y = exp(x)가 단조 증가 함수이기 때문.



신경망을 이용한 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식.

소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않기때문에 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 됨. 

<u>현업에서도 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수를 생략하는 것이 일반적</u>.

 

| NOTE_<br />기계학습의 문제 풀이는 학습과 추론(inference)의 두 단계를 거쳐 이뤄짐. 학습 단계에서 모델을 학습하고, 추론 단계에서 앞서 학습한 모델로 미지의 데이터에 대해서 추론(분류)를 수행함. 앞서 설명한대로 추론 단계에서는 출력층의 소프트맥스 함수를 생략하는 것이 일반적이지만, 학습시킬 때는 출력층에서 소프트맥스 함수를 사용함. |
| ------------------------------------------------------------ |



### 3.5.3 출력층의 뉴런 수 정하기

출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야 함. 

분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적. 

예를 들어 입력 이미지를 숫자 0부터 9중 하나로 분류하는 문제라면 출력층의 뉴런을 10개로 설정.