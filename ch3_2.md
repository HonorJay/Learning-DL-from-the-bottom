## 3.3 다차원 배열의 계산

넘파이의 다차원 배열을 사용한 계산법을 숙달하면 신경망을 효율적으로 구현할 수 있음.



### 3.3.1 다차원 배열

다차원 배열도 그 기본은 '숫자의 집합'.

숫자가 한줄로 늘어선 것이나 직사각형으로 늘어놓은 것, 3차원으로 늘어놓은 것, N차원으로 나열하는 것을 통틀어 다차원 배열이라 함.



1차원 배열의 예

```python
>>> import numpy as np
>>> A = np.array([1, 2, 3, 4])
>>> print(A)
[1, 2, 3, 4]

>>> np.ndim(A)
1

>>> A.shape
(4,)

>>> A.shape[0]
4
```

배열의 차원 수는 np.ndim() 함수로 확인할 수 있음.

또 배열의 형상은 인스턴스 변수인 shape으로 알 수 있음

위 예에서 A는 1차원 배열이고 원소는 4개로 구성되어 있음. 



2차원 배열의 예

```python
>>> B = np.array([[1,2], [3,4], [5,6]])
>>> print(B)
[[1 2]
 [3 4]
 [5 6]]

>>> np.ndim(B)
2

>>> B.shape()
(3, 2)
```

2차원 배열은 **행렬**(matrix)라 부르고 배열의 가로 방향을 **행**(row), 세로 방향을 **열**(column)이라고 함.



### 3.3.2 행렬의 곱

``` python
>>> A = np.array([[1, 2], [3, 4]])
>>> A.shape
(2, 2)

>>> B = np.array([[5, 6], [7, 8]])
>>> B.shape
(2, 2)

>>> np.dot(A, B)
array([[19, 22],
       [43, 50]])
```

A와 B는 2x2 행렬이며, 이 두 행렬의 곱은 넘파이 함수 np.dot()으로 계산.

np.dot()은 입력이 1차원 배열이면 벡터를, 2차원 배열이면 행렬 곱을 계산.

행렬의 곱에서 피연산자의 순서가 다르면 결과도 다르게 나온다는 것을 주의해야 함.



``` python
>>> A = np.array([[1, 2, 3], [4, 5, 6]])
>>> A.shape
(2, 3)

>>> B = np.array([[1, 2], [3, 4], [5, 6]])
>>> B.shape
(3, 2)

>>> np.dot(A, B)
array([[22, 28],
       [49, 64]])
```

이처럼 행렬의 곱을 할때는 '행렬의 형상(shape)'에 주의해야 함. 

행렬 A의 1번째 차원의 원소 수와 행렬 B의 0번째 차원의 원소 수가 같아야 함.

만약 2x3 행렬과 2x2 행렬을 곱하면 파이썬은 다음과 같이 오류를 출력함

```python
>>> C = np.array([[1, 2], [3, 4]])
>>> C.shape
(2, 2)

>>> A.shape
(2, 3)

>>> np.dot(A, C)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    ValueError: shape (2, 3) and (2, 2) not aligned: 3 (dim 1) != 2 (dim 0)
```

이 오류는 행렬 A의 1번째 차원(dim 1)과 행렬 C의 0번째 차원(dim 0)의 원소 수가 다르기 때문에 발생. 즉, 다차원 배열을 곱하려면 두 행렬의 대응하는 차원의 원소 수를 일치시켜야 함.



### 3.3.3 신경망에서의 행렬 곱

``` python
>>> X = np.array([1, 2])
>>> X.shape
(2,)

>>> W = np.array([[1, 3, 5], [2, 4, 6]])
>>> print(W)
[[1 3 5]
 [2 4 6]]

>>> W.shape
(2, 3)

>>> Y = np.dot(X, W)
>>> print(Y)
[5 11 17]
```

다차원 배열의 스칼라곱을 구해주는 np.dot 함수를 사용하면 이처럼 단번에 결과 Y를 계산할 수 있음.

Y의 원소가 100개든 1,000개든 한 번의 연산으로 계산할 수 있음. 



## 3.4 3층 신경망 구현하기

### 3.4.1 표기법 설명

가중치와 은닉층 뉴런의 오른쪽 위에 붙어있는 $^(1)$은 1층의 가중치, 1층의 뉴런임을 뜻하는 번호.

가중치의 오른쪽 아래의 두 숫자는 차례로 다음 층 뉴런과 앞 층 뉴런의 인덱스 번호. 

가령 $w_{12}^{(1)}$은 앞 층의 2번째 뉴런(${x_2}$)에서 다음층의 1번째 뉴런($a_1^{(1)}$)으로 향할 때의 가중치라는 뜻. 



### 3.4.2 각 층의 신호 전달 구현하기

![그림3-17](ch3_2/그림3-17-1625537445717.png)

![수식1](https://raw.githubusercontent.com/HonorJay/images/image/img/수식1.gif)



여기서 행렬의 곱을 이용하면 1층의 '가중치 부분'을 다음 식처럼 간소화 할 수 있음.



![수식2](https://raw.githubusercontent.com/HonorJay/images/image/img/수식2.gif)



이때 행렬 $A^{(1)}$, $X$, $B^{(1)}$, $W^{(1)}은 각각 다음과 같음

![수식3](https://raw.githubusercontent.com/HonorJay/images/image/img/수식3.gif)



![수식4](https://raw.githubusercontent.com/HonorJay/images/image/img/수식4.gif)

``` python
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

>>> print(W1.shape)
(2, 3)

>>> print(X.shape)
(2,)

>>> print(B1.shape)
(3,)

A1 = np.dot(X, W1) + B1
```



![그림3-18](https://raw.githubusercontent.com/HonorJay/images/image/img/그림3-18.png)

은닉층에서의 가중치 합(가중 신호와 편향의 총합)을 a로 표기하고 활성화 함수 h()로 변환된 신호를 z로 표기. 

여기서 활성화 함수로 시그모이드 함수를 사용.

``` python
Z1 = sigmoid(A1)

>>> print(A1)
[0.3, 0.7, 1.1]

>>> print(Z1)
[0.57444252, 0.66818777, 0.75026011]
```



![그림3-19](https://raw.githubusercontent.com/HonorJay/images/image/img/그림3-19.png)

``` python
W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

>>> print(Z1.shape)
(3,)

>>> print(W2.shape)
(3, 2)

>>> print(B2.shape)
(2,)

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)
```



![그림3-20](https://raw.githubusercontent.com/HonorJay/images/image/img/그림3-20.png)

```python
def identity_function(x):
    return x

W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)
```



| NOTE_<br />출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정함. 회귀에는 항등 함수를, 이진분류에는 시그모이드 함수를, 다중 클래스 분류에는 소프트맥스 함수를 사용하는 것이 일반적. |
| ------------------------------------------------------------ |



### 3.4.3 구현 정리

```python
def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    
    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_fuction(a3)
    
    return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)

>>> print(y)
[0.31682708, 0.69627909]
```



init_network() 함수는 가중치와 편향을 초기화하고 딕셔너리 변수인 network에 각 층에 필요한 매개변수(가중치와 편향)를 저장.

forward() 함수는 입력 신호를 출력으로 변환하는 처리과정을 구현.



 